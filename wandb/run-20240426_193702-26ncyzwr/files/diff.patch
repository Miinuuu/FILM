diff --git a/datasets/__init__.py b/datasets/__init__.py
deleted file mode 100644
index 4a3ee1a..0000000
--- a/datasets/__init__.py
+++ /dev/null
@@ -1,3 +0,0 @@
-from .adobe_datasets import *
-from .gopro_datasets import *
-from .vimeo_datasets import *
diff --git a/datasets/__pycache__/__init__.cpython-310.pyc b/datasets/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 8f011a8..0000000
Binary files a/datasets/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/datasets/__pycache__/__init__.cpython-39.pyc b/datasets/__pycache__/__init__.cpython-39.pyc
deleted file mode 100644
index f04ed1b..0000000
Binary files a/datasets/__pycache__/__init__.cpython-39.pyc and /dev/null differ
diff --git a/datasets/__pycache__/adobe_datasets.cpython-310.pyc b/datasets/__pycache__/adobe_datasets.cpython-310.pyc
deleted file mode 100644
index e3e68f1..0000000
Binary files a/datasets/__pycache__/adobe_datasets.cpython-310.pyc and /dev/null differ
diff --git a/datasets/__pycache__/adobe_datasets.cpython-39.pyc b/datasets/__pycache__/adobe_datasets.cpython-39.pyc
deleted file mode 100644
index cbb6748..0000000
Binary files a/datasets/__pycache__/adobe_datasets.cpython-39.pyc and /dev/null differ
diff --git a/datasets/__pycache__/gopro_datasets.cpython-310.pyc b/datasets/__pycache__/gopro_datasets.cpython-310.pyc
deleted file mode 100644
index f5a55f2..0000000
Binary files a/datasets/__pycache__/gopro_datasets.cpython-310.pyc and /dev/null differ
diff --git a/datasets/__pycache__/gopro_datasets.cpython-39.pyc b/datasets/__pycache__/gopro_datasets.cpython-39.pyc
deleted file mode 100644
index c2b69c1..0000000
Binary files a/datasets/__pycache__/gopro_datasets.cpython-39.pyc and /dev/null differ
diff --git a/datasets/__pycache__/vimeo_datasets.cpython-39.pyc b/datasets/__pycache__/vimeo_datasets.cpython-39.pyc
deleted file mode 100644
index 0cf659e..0000000
Binary files a/datasets/__pycache__/vimeo_datasets.cpython-39.pyc and /dev/null differ
diff --git a/datasets/_vimeo_90k_septuplet.py b/datasets/_vimeo_90k_septuplet.py
deleted file mode 100644
index 403ff13..0000000
--- a/datasets/_vimeo_90k_septuplet.py
+++ /dev/null
@@ -1,140 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-import os
-from pathlib import Path
-from typing import Any, Callable, Optional, Sequence, Union
-
-import torch
-from torch import Tensor
-from torch.utils.data import Dataset
-from torchvision.datasets.folder import default_loader
-from torchvision.transforms import ToTensor
-
-
-class Vimeo90kSeptuplet(Dataset):
-    """
-    Loads images or videos from the Vimeo-90k septuplet dataset [1]. The dataset
-    consists of a set of septuplet directories, where each directory contains
-    seven consecutive frames of a video from vimeo.com. Each frame is stored as
-    a PNG file. This class can be configured to return videos (consecutive
-    frames in a septuplet) or individual images every time an item is accessed.
-
-    Xue, Tianfan, et al. "Video enhancement with task-oriented flow."
-    International Journal of Computer Vision 127.8 (2019): 1106-1125.
-
-    Note:
-        Following the conventions of ``torchvision``, in video mode this
-        dataset will have a default transform of
-        ``torchvision.transforms.ToTensor``, while in image mode no default
-        transform is provided.
-
-    Args:
-        root: Path to the Vimeo-90k root directory (i.e. the
-            directory containing the dataset's README).
-        as_video: Determines whether the dataset should return
-            individual images (``as_video=False``) or
-            multiple consecutive frames  (``as_video=True``)
-            at a time.
-        frames_per_group: The number of frames to include from
-            each septuplet. Specifically, the first ``frames_per_group``
-            frames from each septuplet are included in
-            the dataset. Must be between 1 and 7.
-        split: Specifies which dataset parition should be used. Valid values
-            are ``"train"`` or ``"test"``. Exactly one of ``split`` or
-            'folder_list' must be specified.
-        folder_list: A list of paths to septuplets to include in the dataset
-            split. Each septuplet path must be a directory containing the files
-            ``im1.png``, ..., ``im7.png``. Exactly one of ``split`` or
-            ``folder_list`` must be specified.
-        pil_transform: Callable object for applying transforms to
-            the PIL images prior to image concatenation. If using, be sure to
-            have the final operation convert the PIL image to a tensor.
-            Following ``torchvision``'s dataset conventions, the default
-            transform is ``torchvision.transforms.ToTensor`` when
-            in video mode (i.e. when ``as_video=True``), while no
-            default transform is applied in image mode.
-        tensor_transform: Callable object for applying PyTorch
-            transforms after data conversion and septuplet concatenation.
-    """
-
-    def __init__(
-        self,
-        root: Union[str, os.PathLike],
-        as_video: bool = False,
-        frames_per_group: int = 7,
-        split: Optional[str] = None,
-        folder_list: Optional[Sequence[str]] = None,
-        pil_transform: Optional[Callable[[Any], Tensor]] = None,
-        tensor_transform: Optional[Callable[[Tensor], Tensor]] = None,
-    ):
-        self.root = Path(root)
-        self.as_video = as_video
-
-        if frames_per_group not in list(range(1, 8)):
-            raise ValueError(
-                "frames_per_group must be an integer between 1 and 7 (inclusive), "
-                f"not '{frames_per_group}'"
-            )
-
-        self.frames_per_group = frames_per_group
-        self.pil_transform = (
-            ToTensor() if pil_transform is None and as_video else pil_transform
-        )
-        self.tensor_transform = tensor_transform
-
-        if (split is None) == (folder_list is None):
-            raise ValueError("Exactly one of 'split', 'folder_list' must be specified.")
-
-        if folder_list is not None:
-            self.folder_list = folder_list
-        else:
-            if split not in ["train", "test"]:
-                raise ValueError(
-                    "split must take on values of either 'train' or 'test', "
-                    f"not {split}"
-                )
-
-            with open(self.root / f"sep_{split}list.txt", "r") as file:
-                self.folder_list = [
-                    "sequences/" + fname.strip() for fname in file.readlines()
-                ]
-
-    def __len__(self) -> int:
-        if self.as_video:
-            return len(self.folder_list)
-        else:
-            return len(self.folder_list) * self.frames_per_group
-
-    def load_image(self, septuplet: Path, frame_number: int) -> Tensor:
-        img_path = default_loader(str(septuplet / f"im{frame_number}.png"))
-        img = self.pil_transform(img_path)
-        if not isinstance(img, Tensor):
-            raise RuntimeError(
-                "Tensor not returned from pil_transform. "
-                "Did you forget to add ToTensor() to your transform?"
-            )
-        return img
-
-    def __getitem__(self, idx: int) -> Tensor:
-        if self.as_video:
-            folder = self.root / self.folder_list[idx]
-            images = []
-            for im_num in range(1, self.frames_per_group + 1):
-                images.append(self.load_image(folder, im_num))
-
-            item: Tensor = torch.stack(images)
-
-        else:
-            folder_idx = idx // self.frames_per_group
-            frame_idx = idx % self.frames_per_group + 1
-
-            folder = self.root / self.folder_list[folder_idx]
-            item = self.load_image(folder, frame_idx)
-
-        if self.tensor_transform is not None:
-            item = self.tensor_transform(item)
-
-        return item
diff --git a/datasets/adobe_datasets.py b/datasets/adobe_datasets.py
deleted file mode 100755
index a06aee2..0000000
--- a/datasets/adobe_datasets.py
+++ /dev/null
@@ -1,72 +0,0 @@
-'''
-    This code is partially borrowed from IFRNet (https://github.com/ltkong218/IFRNet). 
-'''
-import os
-import sys
-import torch
-import numpy as np
-from torch.utils.data import Dataset
-#sys.path.append('.')
-from utils import read, img2tensor
-from datasets.gopro_datasets import *
-
-
-class Adobe240_Dataset(Dataset):
-    def __init__(self, dataset_dir='data/adobe240/test_frames', interFrames=7, augment=True):
-        super().__init__()
-        self.augment = augment
-        self.interFrames = interFrames
-        self.setLength = interFrames + 2
-        self.dataset_dir = os.path.join(dataset_dir)
-        video_list = os.listdir(self.dataset_dir)[9::10]
-        self.frames_list = []
-        self.file_list = []
-        for video in video_list:
-            frames = sorted(os.listdir(os.path.join(self.dataset_dir, video)))
-            n_sets = (len(frames) - self.setLength) // (interFrames + 1)  + 1
-            videoInputs = [frames[(interFrames + 1) * i: (interFrames + 1) * i + self.setLength] for i in range(n_sets)]
-            videoInputs = [[os.path.join(video, f) for f in group] for group in videoInputs]
-            self.file_list.extend(videoInputs)
-
-    def __getitem__(self, idx):
-        clip_idx = idx // self.interFrames
-        embt_idx = idx % self.interFrames
-        imgpaths = [os.path.join(self.dataset_dir, fp) for fp in self.file_list[clip_idx]]
-        pick_idxs = list(range(0, self.setLength, self.interFrames + 1))
-        imgt_beg = self.setLength // 2 - self.interFrames // 2
-        imgt_end = self.setLength // 2 + self.interFrames // 2 + self.interFrames % 2
-        imgt_idx = list(range(imgt_beg, imgt_end)) 
-        input_paths = [imgpaths[idx] for idx in pick_idxs]
-        imgt_paths = [imgpaths[idx] for idx in imgt_idx]
-        
-        img0 = np.array(read(input_paths[0]))
-        imgt = np.array(read(imgt_paths[embt_idx]))
-        img1 = np.array(read(input_paths[1]))
-        embt = torch.from_numpy(np.array((embt_idx  + 1) / (self.interFrames + 1)
-                                         ).reshape(1, 1, 1).astype(np.float32))
-
-        if self.augment == True:
-            img0, imgt, img1 = random_resize_woflow(img0, imgt, img1, p=0.1)
-            img0, imgt, img1 = random_crop_woflow(img0, imgt, img1, crop_size=(224, 224))
-            img0, imgt, img1 = random_reverse_channel_woflow(img0, imgt, img1, p=0.5)
-            img0, imgt, img1 = random_vertical_flip_woflow(img0, imgt, img1, p=0.3)
-            img0, imgt, img1 = random_horizontal_flip_woflow(img0, imgt, img1, p=0.5)
-            img0, imgt, img1 = random_rotate_woflow(img0, imgt, img1, p=0.05)
-            img0, imgt, img1, embt = random_reverse_time_woflow(img0, imgt, img1, 
-                                                                embt=embt, p=0.5)
-        else:
-            img0, imgt, img1 = center_crop_woflow(img0, imgt, img1, crop_size=(512, 512))
-            
-        img0 = img2tensor(img0).squeeze(0)
-        imgt = img2tensor(imgt).squeeze(0)
-        img1 = img2tensor(img1).squeeze(0)
-        
-        return torch.cat((img0,img1,imgt),0)
-    
-        return {'img0': img0.float(), 
-                'imgt': imgt.float(), 
-                'img1': img1.float(),  
-                'embt': embt}
-
-    def __len__(self):
-        return len(self.file_list) * self.interFrames
diff --git a/datasets/create_vimeo90K_tfrecord.py b/datasets/create_vimeo90K_tfrecord.py
deleted file mode 100644
index b54a137..0000000
--- a/datasets/create_vimeo90K_tfrecord.py
+++ /dev/null
@@ -1,178 +0,0 @@
-# Copyright 2022 Google LLC
-
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-
-#     https://www.apache.org/licenses/LICENSE-2.0
-
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-r"""Beam pipeline that generates Vimeo-90K (train or test) triplet TFRecords.
-
-Vimeo-90K dataset is built upon 5,846 videos downloaded from vimeo.com. The list
-of the original video links are available here:
-https://github.com/anchen1011/toflow/blob/master/data/original_vimeo_links.txt.
-Each video is further cropped into a fixed spatial size of (448 x 256) to create
-89,000 video clips.
-
-The Vimeo-90K dataset is designed for four video processing tasks. This script
-creates the TFRecords of frame triplets for frame interpolation task.
-
-Temporal frame interpolation triplet dataset:
-  - 73,171 triplets of size (448x256) extracted from 15K subsets of Vimeo-90K.
-  - The triplets are pre-split into (train,test) = (51313,3782)
-  - Download links:
-    Test-set: http://data.csail.mit.edu/tofu/testset/vimeo_interp_test.zip
-    Train+test-set: http://data.csail.mit.edu/tofu/dataset/vimeo_triplet.zip
-
-For more information, see the arXiv paper, project page or the GitHub link.
-@article{xue17toflow,
-  author = {Xue, Tianfan and
-            Chen, Baian and
-            Wu, Jiajun and
-            Wei, Donglai and
-            Freeman, William T},
-  title = {Video Enhancement with Task-Oriented Flow},
-  journal = {arXiv},
-  year = {2017}
-}
-Project: http://toflow.csail.mit.edu/
-GitHub: https://github.com/anchen1011/toflow
-
-Inputs to the script are (1) the directory to the downloaded and unzipped folder
-(2) the filepath of the text-file that lists the subfolders of the triplets.
-
-Output TFRecord is a tf.train.Example proto of each image triplet.
-The feature_map takes the form:
-  feature_map {
-      'frame_0/encoded':
-          tf.io.FixedLenFeature((), tf.string, default_value=''),
-      'frame_0/format':
-          tf.io.FixedLenFeature((), tf.string, default_value='jpg'),
-      'frame_0/height':
-          tf.io.FixedLenFeature((), tf.int64, default_value=0),
-      'frame_0/width':
-          tf.io.FixedLenFeature((), tf.int64, default_value=0),
-      'frame_1/encoded':
-          tf.io.FixedLenFeature((), tf.string, default_value=''),
-      'frame_1/format':
-          tf.io.FixedLenFeature((), tf.string, default_value='jpg'),
-      'frame_1/height':
-          tf.io.FixedLenFeature((), tf.int64, default_value=0),
-      'frame_1/width':
-          tf.io.FixedLenFeature((), tf.int64, default_value=0),
-      'frame_2/encoded':
-          tf.io.FixedLenFeature((), tf.string, default_value=''),
-      'frame_2/format':
-          tf.io.FixedLenFeature((), tf.string, default_value='jpg'),
-      'frame_2/height':
-          tf.io.FixedLenFeature((), tf.int64, default_value=0),
-      'frame_2/width':
-          tf.io.FixedLenFeature((), tf.int64, default_value=0)
-      'path':
-          tf.io.FixedLenFeature((), tf.string, default_value='')
-  }
-
-Usage example:
-  python3 -m frame_interpolation.datasets.create_vimeo90K_tfrecord \
-    --input_dir=<root folder of vimeo90K dataset> \
-    --input_triplet_list_filepath=<filepath of tri_{test|train}list.txt> \
-    --output_tfrecord_filepath=<output tfrecord filepath>
-
-    python3 -m frame_interpolation.datasets.create_vimeo90K_tfrecord \
-    --input_dir=/data/dataset/vimeo_dataset/vimeo_triplet/sequences \
-    --input_triplet_list_filepath=/data/dataset/vimeo_dataset/vimeo_triplet/tri_trainlist.txt \
-    --output_tfrecord_filepath=/data/dataset/vimeo_dataset/vimeo_triplet/tf_train
-
-  python3 -m frame_interpolation.datasets.create_vimeo90K_tfrecord \
-    --input_dir=/data/dataset/vimeo_dataset/vimeo_triplet/sequences\
-    --input_triplet_list_filepath=/data/dataset/vimeo_dataset/vimeo_triplet/tri_testlist.txt \
-    --output_tfrecord_filepath=/data/dataset/vimeo_dataset/vimeo_triplet/tf_val/
-
-"""
-import os
-
-from util import *
-from absl import app
-from absl import flags
-from absl import logging
-import apache_beam as beam
-import numpy as np
-import tensorflow as tf
-
-
-_INPUT_DIR = flags.DEFINE_string(
-    'input_dir',
-    default='/data/dataset/vimeo_dataset/vimeo_triplet/sequences',
-    help='Path to the root directory of the vimeo frame interpolation dataset. '
-    'We expect the data to have been downloaded and unzipped.\n'
-    'Folder structures:\n'
-    '| raw_vimeo_dataset/\n'
-    '|  sequences/\n'
-    '|  |  00001\n'
-    '|  |  |  0389/\n'
-    '|  |  |  |  im1.png\n'
-    '|  |  |  |  im2.png\n'
-    '|  |  |  |  im3.png\n'
-    '|  |  |  ...\n'
-    '|  |  00002/\n'
-    '|  |  ...\n'
-    '|  readme.txt\n'
-    '|  tri_trainlist.txt\n'
-    '|  tri_testlist.txt \n')
-
-_INTPUT_TRIPLET_LIST_FILEPATH = flags.DEFINE_string(
-    'input_triplet_list_filepath',
-    default='/data/dataset/vimeo_dataset/vimeo_triplet/tri_trainlist.txt',
-    help='Text file containing a list of sub-directories of input triplets.')
-
-_OUTPUT_TFRECORD_FILEPATH = flags.DEFINE_string(
-    'output_tfrecord_filepath',
-    default='/data/dataset/vimeo_dataset/vimeo_triplet/tf_train',
-    help='Filepath to the output TFRecord file.')
-
-_NUM_SHARDS = flags.DEFINE_integer('num_shards',
-    default=200, # set to 3 for vimeo_test, and 200 for vimeo_train.
-    help='Number of shards used for the output.')
-
-# Image key -> basename for frame interpolator: start / middle / end frames.
-_INTERPOLATOR_IMAGES_MAP = {
-    'frame_0': 'im1.png',
-    'frame_1': 'im2.png',
-    'frame_2': 'im3.png',
-}
-
-
-def main(unused_argv):
-  """Creates and runs a Beam pipeline to write frame triplets as a TFRecord."""
-  with tf.io.gfile.GFile(_INTPUT_TRIPLET_LIST_FILEPATH.value, 'r') as fid:
-    triplets_list = np.loadtxt(fid, dtype=str)
-
-  triplet_dicts = []
-  for triplet in triplets_list:
-    triplet_dict = {
-        image_key: os.path.join(_INPUT_DIR.value, triplet, image_basename)
-        for image_key, image_basename in _INTERPOLATOR_IMAGES_MAP.items()
-    }
-    triplet_dicts.append(triplet_dict)
-  p = beam.Pipeline('DirectRunner')
-  (p | 'ReadInputTripletDicts' >> beam.Create(triplet_dicts)  # pylint: disable=expression-not-assigned
-   | 'GenerateSingleExample' >> beam.ParDo(
-       ExampleGenerator(_INTERPOLATOR_IMAGES_MAP))
-   | 'WriteToTFRecord' >> beam.io.tfrecordio.WriteToTFRecord(
-       file_path_prefix=_OUTPUT_TFRECORD_FILEPATH.value,
-       num_shards=_NUM_SHARDS.value,
-       coder=beam.coders.BytesCoder()))
-  result = p.run()
-  result.wait_until_finish()
-
-  logging.info('Succeeded in creating the output TFRecord file: \'%s@%s\'.',
-    _OUTPUT_TFRECORD_FILEPATH.value, str(_NUM_SHARDS.value))
-
-if __name__ == '__main__':
-  app.run(main)
diff --git a/datasets/gopro_datasets.py b/datasets/gopro_datasets.py
deleted file mode 100755
index 8bd4cb3..0000000
--- a/datasets/gopro_datasets.py
+++ /dev/null
@@ -1,190 +0,0 @@
-'''
-    This code is partially borrowed from IFRNet (https://github.com/ltkong218/IFRNet). 
-    In the consideration of the difficulty in flow supervision generation, we abort 
-    flow loss in the 8x case.
-'''
-import os
-import cv2
-import torch
-import random
-import numpy as np
-from torch.utils.data import Dataset
-from utils import read, img2tensor
-
-def random_resize_woflow(img0, imgt, img1, p=0.1):
-    if random.uniform(0, 1) < p:
-        img0 = cv2.resize(img0, dsize=None, fx=2.0, fy=2.0, interpolation=cv2.INTER_LINEAR)
-        imgt = cv2.resize(imgt, dsize=None, fx=2.0, fy=2.0, interpolation=cv2.INTER_LINEAR)
-        img1 = cv2.resize(img1, dsize=None, fx=2.0, fy=2.0, interpolation=cv2.INTER_LINEAR)
-    return img0, imgt, img1
-
-def random_crop_woflow(img0, imgt, img1, crop_size=(224, 224)):
-    h, w = crop_size[0], crop_size[1]
-    ih, iw, _ = img0.shape
-    x = np.random.randint(0, ih-h+1)
-    y = np.random.randint(0, iw-w+1)
-    img0 = img0[x: x + h, y : y + w, :]
-    imgt = imgt[x: x + h, y : y + w, :]
-    img1 = img1[x: x + h, y : y + w, :]
-    return img0, imgt, img1
-
-def center_crop_woflow(img0, imgt, img1, crop_size=(512, 512)):
-    h, w = crop_size[0], crop_size[1]
-    ih, iw, _ = img0.shape
-    img0 = img0[ih // 2 - h // 2: ih // 2 + h // 2, iw // 2 - w // 2: iw // 2 +  w // 2, :]
-    imgt = imgt[ih // 2 - h // 2: ih // 2 + h // 2, iw // 2 - w // 2: iw // 2 +  w // 2, :]
-    img1 = img1[ih // 2 - h // 2: ih // 2 + h // 2, iw // 2 - w // 2: iw // 2 +  w // 2, :]
-    return img0, imgt, img1
-
-def random_reverse_channel_woflow(img0, imgt, img1, p=0.5):
-    if random.uniform(0, 1) < p:
-        img0 = img0[:, :, ::-1]
-        imgt = imgt[:, :, ::-1]
-        img1 = img1[:, :, ::-1]
-    return img0, imgt, img1
-
-def random_vertical_flip_woflow(img0, imgt, img1, p=0.3):
-    if random.uniform(0, 1) < p:
-        img0 = img0[::-1]
-        imgt = imgt[::-1]
-        img1 = img1[::-1]
-    return img0, imgt, img1
-
-def random_horizontal_flip_woflow(img0, imgt, img1, p=0.5):
-    if random.uniform(0, 1) < p:
-        img0 = img0[:, ::-1]
-        imgt = imgt[:, ::-1]
-        img1 = img1[:, ::-1]
-    return img0, imgt, img1
-
-def random_rotate_woflow(img0, imgt, img1, p=0.05):
-    if random.uniform(0, 1) < p:
-        img0 = img0.transpose((1, 0, 2))
-        imgt = imgt.transpose((1, 0, 2))
-        img1 = img1.transpose((1, 0, 2))
-    return img0, imgt, img1
-
-def random_reverse_time_woflow(img0, imgt, img1, embt, p=0.5):
-    if random.uniform(0, 1) < p:
-        tmp = img1
-        img1 = img0
-        img0 = tmp
-    embt = 1 - embt
-    return img0, imgt, img1, embt
-
-class GoPro_Train_Dataset(Dataset):
-    def __init__(self, dataset_dir='/data/dataset/GOPRO_Large_all', interFrames=7, augment=True):
-        self.dataset_dir = dataset_dir + '/train'
-        self.interFrames = interFrames
-        self.augment = augment
-        self.setLength = interFrames + 2
-        video_list = [
-            'GOPR0372_07_00', 'GOPR0374_11_01', 'GOPR0378_13_00', 'GOPR0384_11_01', 
-            'GOPR0384_11_04', 'GOPR0477_11_00', 'GOPR0868_11_02', 'GOPR0884_11_00', 
-            'GOPR0372_07_01', 'GOPR0374_11_02', 'GOPR0379_11_00', 'GOPR0384_11_02', 
-            'GOPR0385_11_00', 'GOPR0857_11_00', 'GOPR0871_11_01', 'GOPR0374_11_00', 
-            'GOPR0374_11_03', 'GOPR0380_11_00', 'GOPR0384_11_03', 'GOPR0386_11_00', 
-            'GOPR0868_11_01', 'GOPR0881_11_00']
-        self.frames_list = []
-        self.file_list = []
-        for video in video_list:
-            frames = sorted(os.listdir(os.path.join(self.dataset_dir, video)))
-            n_sets = (len(frames) - self.setLength) // (interFrames+1)  + 1
-            videoInputs = [frames[(interFrames + 1) * i: (interFrames + 1) * i + self.setLength
-                                                        ] for i in range(n_sets)]
-            videoInputs = [[os.path.join(video, f) for f in group] for group in videoInputs]
-            self.file_list.extend(videoInputs)
-
-    def __len__(self):
-        return len(self.file_list) * self.interFrames
-
-    def __getitem__(self, idx):
-        clip_idx = idx // self.interFrames
-        embt_idx = idx % self.interFrames
-        imgpaths = [os.path.join(self.dataset_dir, fp) for fp in self.file_list[clip_idx]]
-        pick_idxs = list(range(0, self.setLength, self.interFrames + 1))
-        imgt_beg = self.setLength // 2 - self.interFrames // 2
-        imgt_end = self.setLength // 2 + self.interFrames // 2 + self.interFrames % 2
-        imgt_idx = list(range(imgt_beg, imgt_end)) 
-        input_paths = [imgpaths[idx] for idx in pick_idxs]
-        imgt_paths = [imgpaths[idx] for idx in imgt_idx]
-        
-        embt = torch.from_numpy(np.array((embt_idx  + 1) / (self.interFrames+1)
-                                         ).reshape(1, 1, 1).astype(np.float32))
-        img0 = np.array(read(input_paths[0]))
-        imgt = np.array(read(imgt_paths[embt_idx]))
-        img1 = np.array(read(input_paths[1]))
-
-        if self.augment == True:
-            img0, imgt, img1 = random_resize_woflow(img0, imgt, img1, p=0.1)
-            img0, imgt, img1 = random_crop_woflow(img0, imgt, img1, crop_size=(224, 224))
-            img0, imgt, img1 = random_reverse_channel_woflow(img0, imgt, img1, p=0.5)
-            img0, imgt, img1 = random_vertical_flip_woflow(img0, imgt, img1, p=0.3)
-            img0, imgt, img1 = random_horizontal_flip_woflow(img0, imgt, img1, p=0.5)
-            img0, imgt, img1 = random_rotate_woflow(img0, imgt, img1, p=0.05)
-            img0, imgt, img1, embt = random_reverse_time_woflow(img0, imgt, img1, 
-                                                                embt=embt, p=0.5)
-        else:
-            img0, imgt, img1 = center_crop_woflow(img0, imgt, img1, crop_size=(512, 512))
-            
-        img0 = img2tensor(img0.copy()).squeeze(0)
-        imgt = img2tensor(imgt.copy()).squeeze(0)
-        img1 = img2tensor(img1.copy()).squeeze(0)
-        
-        return torch.cat((img0,img1,imgt),0)
-        return {'img0': img0.float(), 
-                'imgt': imgt.float(), 
-                'img1': img1.float(),  
-                'embt': embt}
-
-class GoPro_Test_Dataset(Dataset):
-    def __init__(self, dataset_dir='/data/dataset/GOPRO_Large', interFrames=7):
-        self.dataset_dir = dataset_dir + '/test'
-        self.interFrames = interFrames
-        self.setLength = interFrames + 2
-        video_list = [
-            'GOPR0384_11_00', 'GOPR0385_11_01', 'GOPR0410_11_00', 
-            'GOPR0862_11_00', 'GOPR0869_11_00', 'GOPR0881_11_01', 
-            'GOPR0384_11_05', 'GOPR0396_11_00', 'GOPR0854_11_00', 
-            'GOPR0868_11_00', 'GOPR0871_11_00']
-        self.frames_list = []
-        self.file_list = []
-        for video in video_list:
-            frames = sorted(os.listdir(os.path.join(self.dataset_dir, video)))
-            n_sets = (len(frames) - self.setLength)//(interFrames+1)  + 1
-            videoInputs = [frames[(interFrames + 1) * i:(interFrames + 1) * i + self.setLength
-                                                        ] for i in range(n_sets)]
-            videoInputs = [[os.path.join(video, f) for f in group] for group in videoInputs]
-            self.file_list.extend(videoInputs)
-
-    def __len__(self):
-        return len(self.file_list) * self.interFrames
-
-    def __getitem__(self, idx):
-        clip_idx = idx // self.interFrames
-        embt_idx = idx % self.interFrames
-        imgpaths = [os.path.join(self.dataset_dir, fp) for fp in self.file_list[clip_idx]]
-        pick_idxs = list(range(0, self.setLength, self.interFrames + 1))
-        imgt_beg = self.setLength // 2 - self.interFrames // 2
-        imgt_end = self.setLength // 2 + self.interFrames // 2 + self.interFrames % 2
-        imgt_idx = list(range(imgt_beg, imgt_end)) 
-        input_paths = [imgpaths[idx] for idx in pick_idxs]
-        imgt_paths = [imgpaths[idx] for idx in imgt_idx]
-
-        img0 = np.array(read(input_paths[0]))
-        imgt = np.array(read(imgt_paths[embt_idx]))
-        img1 = np.array(read(input_paths[1]))
-
-        img0, imgt, img1 = center_crop_woflow(img0, imgt, img1, crop_size=(512, 512))
-
-        img0 = img2tensor(img0).squeeze(0)
-        imgt = img2tensor(imgt).squeeze(0)
-        img1 = img2tensor(img1).squeeze(0)
-        
-        embt = torch.from_numpy(np.array((embt_idx + 1) / (self.interFrames + 1)
-                                         ).reshape(1, 1, 1).astype(np.float32))
-        return torch.cat((img0,img1,imgt),0)
-        return {'img0': img0.float(), 
-                'imgt': imgt.float(), 
-                'img1': img1.float(),  
-                'embt': embt}
\ No newline at end of file
diff --git a/datasets/vimeo90k_septuplet.py b/datasets/vimeo90k_septuplet.py
deleted file mode 100644
index aa62464..0000000
--- a/datasets/vimeo90k_septuplet.py
+++ /dev/null
@@ -1,112 +0,0 @@
-import os
-from torch.utils.data import Dataset, DataLoader
-from torchvision import transforms
-from PIL import Image
-import random
-
-class VimeoSepTuplet(Dataset):
-    def __init__(self, data_root, is_training , input_frames="1357", mode='mini'):
-        """
-        Creates a Vimeo Septuplet object.
-        Inputs.
-            data_root: Root path for the Vimeo dataset containing the sep tuples.
-            is_training: Train/Test.
-            input_frames: Which frames to input for frame interpolation network.
-        """
-        self.data_root = data_root
-        self.image_root = os.path.join(self.data_root, 'sequences')
-        self.training = is_training
-        self.inputs = input_frames
-
-        train_fn = os.path.join(self.data_root, 'sep_trainlist.txt')
-        test_fn = os.path.join(self.data_root, 'sep_testlist.txt')
-        with open(train_fn, 'r') as f:
-            self.trainlist = f.read().splitlines()
-        with open(test_fn, 'r') as f:
-            self.testlist = f.read().splitlines()
-
-        if mode != 'full':
-            tmp = []
-            for i, value in enumerate(self.testlist):
-                if i % 38 == 0:
-                    tmp.append(value)
-            self.testlist = tmp
-
-        if self.training:
-            self.transforms = transforms.Compose([
-                transforms.RandomCrop(256),
-                transforms.RandomHorizontalFlip(0.5),
-                transforms.RandomVerticalFlip(0.5),
-                # transforms.ColorJitter(0.05, 0.05, 0.05, 0.05),
-                transforms.ToTensor()
-            ])
-        else:
-            self.transforms = transforms.Compose([
-                transforms.ToTensor()
-            ])
-
-    def __getitem__(self, index):
-        if self.training:
-            imgpath = os.path.join(self.image_root, self.trainlist[index])
-        else:
-            imgpath = os.path.join(self.image_root, self.testlist[index])
-
-        imgpaths = [imgpath + f'/im{i}.png' for i in range(1,8)]
-
-        pth_ = imgpaths
-
-        # Load images
-        images = [Image.open(pth) for pth in imgpaths]
-
-        ## Select only relevant inputs
-        inputs = [int(e)-1 for e in list(self.inputs)]
-        inputs = inputs[:len(inputs)//2] + [3] + inputs[len(inputs)//2:]
-        images = [images[i] for i in inputs]
-        imgpaths = [imgpaths[i] for i in inputs]
-        # Data augmentation
-        if self.training:
-            seed = random.randint(0, 2**32)
-            images_ = []
-            for img_ in images:
-                random.seed(seed)
-                images_.append(self.transforms(img_))
-            images = images_
-            # Random Temporal Flip
-            if random.random() >= 0.5:
-                images = images[::-1]
-                imgpaths = imgpaths[::-1]
-            gt = images[len(images) // 2]
-            images = images[:len(images) // 2] + images[len(images) // 2 + 1:]
-
-            return images, gt
-        else:
-            T = self.transforms
-            images = [T(img_) for img_ in images]
-
-            gt = images[len(images)//2]
-            images = images[:len(images)//2] + images[len(images)//2+1:]
-            imgpath = '_'.join(imgpath.split('/')[-2:])
-
-            return images, gt, imgpath
-
-    def __len__(self):
-        if self.training:
-            return len(self.trainlist)
-        else:
-            return len(self.testlist)
-            # return 1
-
-def get_loader(mode, data_root, batch_size, shuffle, num_workers, test_mode=None):
-    if mode == 'train':
-        is_training = True
-    else:
-        is_training = False
-    dataset = VimeoSepTuplet(data_root, is_training=is_training)
-    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)
-
-
-if __name__ == "__main__":
-
-    dataset = VimeoSepTuplet("/home/zhihao/DATA-M2/video_interpolation//vimeo_septuplet/", is_training=True)
-    print(dataset[0])
-    dataloader = DataLoader(dataset, batch_size=100, shuffle=False, num_workers=32, pin_memory=True)
\ No newline at end of file
diff --git a/datasets/vimeo_datasets.py b/datasets/vimeo_datasets.py
deleted file mode 100644
index 090982c..0000000
--- a/datasets/vimeo_datasets.py
+++ /dev/null
@@ -1,128 +0,0 @@
-import cv2
-import os
-import numpy as np
-import random
-from tensorflow.keras.utils import Sequence
-import tensorflow as tf
-
-import math
-cv2.setNumThreads(1)
-class VimeoDataset(Sequence):
-    def __init__(self, dataset_name, path, batch_size=32, shuffle=False, train_root='tri_trainlist.txt',test_root='tri_testlist.txt'):
-        self.batch_size = batch_size
-        self.dataset_name = dataset_name
-        self.h = 256
-        self.w = 448
-        self.data_root = path
-        self.shuffle=shuffle
-        self.image_root = os.path.join(self.data_root, 'sequences')
-        train_fn = os.path.join(self.data_root, train_root)
-        test_fn = os.path.join(self.data_root, test_root)
-        with open(train_fn, 'r') as f:
-            self.trainlist = f.read().splitlines()
-        with open(test_fn, 'r') as f:
-            self.testlist = f.read().splitlines()                                                    
-        self.load_data()
-        self.on_epoch_end()
-
-    def __len__(self):
-        #return len(self.meta_data)
-        return math.ceil(len(self.meta_data) /self.batch_size)
-
-    def load_data(self):
-        if self.dataset_name != 'test':
-            self.meta_data = self.trainlist
-        else:
-            self.meta_data = self.testlist
-
-    def aug(self, img0, gt, img1, h, w):
-        ih, iw, _ = img0.shape
-        x = np.random.randint(0, ih - h + 1)
-        y = np.random.randint(0, iw - w + 1)
-        img0 = img0[x:x+h, y:y+w, :]
-        img1 = img1[x:x+h, y:y+w, :]
-        gt = gt[x:x+h, y:y+w, :]
-        return img0, gt, img1
-    
-    def on_epoch_end(self):
-        self.indices = np.arange(len(self.meta_data))
-        if self.shuffle == True:
-            np.random.shuffle(self.indices)
-    
-    def getimg(self, indices):
-        img0=[]
-        img1=[]
-        gt=[]
-        for index in indices:
-            imgpath = os.path.join(self.image_root, self.meta_data[index]) 
-            imgpaths = [imgpath + '/im1.png', imgpath + '/im2.png', imgpath + '/im3.png']
-            #image = tf.io.read_file(imgpaths[0])
-            #image = tf.image.decode_png(image)
-            #print('tf',image)
-            img0.append(cv2.imread(imgpaths[0]))
-            gt.append(cv2.imread(imgpaths[1]))
-            img1.append(cv2.imread(imgpaths[2]))
-
-        #H=512
-        #W=512
-        #img0 = cv2.resize(img0,(W,H))#1k
-        #gt = cv2.resize(gt,(W,H))
-        #img1 = cv2.resize(img1,(W,H))
-
-        return np.array(img0), np.array(gt), np.array(img1)
-            
-    def __getitem__(self, idx):
-
-        indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]                        
-        
-        img0, gt, img1 = self.getimg(indices)
-                
-        if 'train' in self.dataset_name:
-            img0, gt, img1 = self.aug(img0, gt, img1, 256, 256)
-            if random.uniform(0, 1) < 0.5:
-                img0 = img0[:, :, ::-1]
-                img1 = img1[:, :, ::-1]
-                gt = gt[:, :, ::-1]
-            if random.uniform(0, 1) < 0.5:
-                img1, img0 = img0, img1
-            if random.uniform(0, 1) < 0.5:
-                img0 = img0[::-1]
-                img1 = img1[::-1]
-                gt = gt[::-1]
-            if random.uniform(0, 1) < 0.5:
-                img0 = img0[:, ::-1]
-                img1 = img1[:, ::-1]
-                gt = gt[:, ::-1]
-
-            p = random.uniform(0, 1)
-            if p < 0.25:
-                img0 = cv2.rotate(img0, cv2.ROTATE_90_CLOCKWISE)
-                gt = cv2.rotate(gt, cv2.ROTATE_90_CLOCKWISE)
-                img1 = cv2.rotate(img1, cv2.ROTATE_90_CLOCKWISE)
-            elif p < 0.5:
-                img0 = cv2.rotate(img0, cv2.ROTATE_180)
-                gt = cv2.rotate(gt, cv2.ROTATE_180)
-                img1 = cv2.rotate(img1, cv2.ROTATE_180)
-            elif p < 0.75:
-                img0 = cv2.rotate(img0, cv2.ROTATE_90_COUNTERCLOCKWISE)
-                gt = cv2.rotate(gt, cv2.ROTATE_90_COUNTERCLOCKWISE)
-                img1 = cv2.rotate(img1, cv2.ROTATE_90_COUNTERCLOCKWISE)
-        #Y = 0.299R + 0.587G + 0.114B
-        #img0 = torch.from_numpy(img0.copy()).permute(2, 0, 1)
-        #img1 = torch.from_numpy(img1.copy()).permute(2, 0, 1)
-        #gt = torch.from_numpy(gt.copy()).permute(2, 0, 1)
-        
-        #img0 = 0.299* img0[ 0:1, : , :] + 0.587*img0[ 1:2, : , :] + 0.114*img0[ 2:3, : , :] 
-        #img1 = 0.299* img1[ 0:1, : , :] + 0.587*img1[ 1:2, : , :] + 0.114*img1[ 2:3, : , :] 
-        #gt = 0.299* gt[ 0:1, : , :] + 0.587*gt[ 1:2, : , :] + 0.114*gt[ 2:3, : , :] 
-        return img0,gt, img1
-        #return torch.cat((img0, img1, gt), 0)
-
-if __name__ == '__main__':
-    test_loader=VimeoDataset(dataset_name='test',
-               path='/data/dataset/vimeo_dataset/vimeo_triplet',
-               shuffle=True)
-    for e in range(10):
-        for img0,gt,img1 in test_loader:
-            pass
-            print(img0, img0.shape)
\ No newline at end of file
diff --git a/src/__pycache__/basic.cpython-39.pyc b/src/__pycache__/basic.cpython-39.pyc
deleted file mode 100644
index bebb6a7..0000000
Binary files a/src/__pycache__/basic.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/config.cpython-39.pyc b/src/__pycache__/config.cpython-39.pyc
deleted file mode 100644
index e693068..0000000
Binary files a/src/__pycache__/config.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/feature_extractor.cpython-39.pyc b/src/__pycache__/feature_extractor.cpython-39.pyc
deleted file mode 100644
index faa79f4..0000000
Binary files a/src/__pycache__/feature_extractor.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/fusion.cpython-39.pyc b/src/__pycache__/fusion.cpython-39.pyc
deleted file mode 100644
index 30a9df3..0000000
Binary files a/src/__pycache__/fusion.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/laploss.cpython-39.pyc b/src/__pycache__/laploss.cpython-39.pyc
deleted file mode 100644
index ba13f6d..0000000
Binary files a/src/__pycache__/laploss.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/metric_collection.cpython-39.pyc b/src/__pycache__/metric_collection.cpython-39.pyc
deleted file mode 100644
index 51b8548..0000000
Binary files a/src/__pycache__/metric_collection.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/model.cpython-39.pyc b/src/__pycache__/model.cpython-39.pyc
deleted file mode 100644
index c8a4caf..0000000
Binary files a/src/__pycache__/model.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/models_test.cpython-39.pyc b/src/__pycache__/models_test.cpython-39.pyc
deleted file mode 100644
index 31b1bc9..0000000
Binary files a/src/__pycache__/models_test.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/modules.cpython-39.pyc b/src/__pycache__/modules.cpython-39.pyc
deleted file mode 100644
index 7e9862a..0000000
Binary files a/src/__pycache__/modules.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/options.cpython-39.pyc b/src/__pycache__/options.cpython-39.pyc
deleted file mode 100644
index c767739..0000000
Binary files a/src/__pycache__/options.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/pyramid_flow_estimator.cpython-39.pyc b/src/__pycache__/pyramid_flow_estimator.cpython-39.pyc
deleted file mode 100644
index f168748..0000000
Binary files a/src/__pycache__/pyramid_flow_estimator.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/refine.cpython-39.pyc b/src/__pycache__/refine.cpython-39.pyc
deleted file mode 100644
index 04148c8..0000000
Binary files a/src/__pycache__/refine.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/schedule.cpython-39.pyc b/src/__pycache__/schedule.cpython-39.pyc
deleted file mode 100644
index f30abad..0000000
Binary files a/src/__pycache__/schedule.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/tf_memoize.cpython-39.pyc b/src/__pycache__/tf_memoize.cpython-39.pyc
deleted file mode 100644
index cdb191e..0000000
Binary files a/src/__pycache__/tf_memoize.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/util.cpython-39.pyc b/src/__pycache__/util.cpython-39.pyc
deleted file mode 100644
index d601ba4..0000000
Binary files a/src/__pycache__/util.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/video_tensors.cpython-39.pyc b/src/__pycache__/video_tensors.cpython-39.pyc
deleted file mode 100644
index 75d6309..0000000
Binary files a/src/__pycache__/video_tensors.cpython-39.pyc and /dev/null differ
diff --git a/src/__pycache__/vimeo_datasets.cpython-39.pyc b/src/__pycache__/vimeo_datasets.cpython-39.pyc
deleted file mode 100644
index 775391c..0000000
Binary files a/src/__pycache__/vimeo_datasets.cpython-39.pyc and /dev/null differ
diff --git a/src/feature_extractor.py b/src/feature_extractor.py
deleted file mode 100644
index c1b9d81..0000000
--- a/src/feature_extractor.py
+++ /dev/null
@@ -1,193 +0,0 @@
-# Copyright 2022 Google LLC
-
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-
-#     https://www.apache.org/licenses/LICENSE-2.0
-
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""TF2 layer for extracting image features for the film_net interpolator.
-
-The feature extractor implemented here converts an image pyramid into a pyramid
-of deep features. The feature pyramid serves a similar purpose as U-Net
-architecture's encoder, but we use a special cascaded architecture described in
-Multi-view Image Fusion [1].
-
-For comprehensiveness, below is a short description of the idea. While the
-description is a bit involved, the cascaded feature pyramid can be used just
-like any image feature pyramid.
-
-Why cascaded architeture?
-=========================
-To understand the concept it is worth reviewing a traditional feature pyramid
-first: *A traditional feature pyramid* as in U-net or in many optical flow
-networks is built by alternating between convolutions and pooling, starting
-from the input image.
-
-It is well known that early features of such architecture correspond to low
-level concepts such as edges in the image whereas later layers extract
-semantically higher level concepts such as object classes etc. In other words,
-the meaning of the filters in each resolution level is different. For problems
-such as semantic segmentation and many others this is a desirable property.
-
-However, the asymmetric features preclude sharing weights across resolution
-levels in the feature extractor itself and in any subsequent neural networks
-that follow. This can be a downside, since optical flow prediction, for
-instance is symmetric across resolution levels. The cascaded feature
-architecture addresses this shortcoming.
-
-How is it built?
-================
-The *cascaded* feature pyramid contains feature vectors that have constant
-length and meaning on each resolution level, except few of the finest ones. The
-advantage of this is that the subsequent optical flow layer can learn
-synergically from many resolutions. This means that coarse level prediction can
-benefit from finer resolution training examples, which can be useful with
-moderately sized datasets to avoid overfitting.
-
-The cascaded feature pyramid is built by extracting shallower subtree pyramids,
-each one of them similar to the traditional architecture. Each subtree
-pyramid S_i is extracted starting from each resolution level:
-
-image resolution 0 -> S_0
-image resolution 1 -> S_1
-image resolution 2 -> S_2
-...
-
-If we denote the features at level j of subtree i as S_i_j, the cascaded pyramid
-is constructed by concatenating features as follows (assuming subtree depth=3):
-
-lvl
-feat_0 = concat(                               S_0_0 )
-feat_1 = concat(                         S_1_0 S_0_1 )
-feat_2 = concat(                   S_2_0 S_1_1 S_0_2 )
-feat_3 = concat(             S_3_0 S_2_1 S_1_2       )
-feat_4 = concat(       S_4_0 S_3_1 S_2_2             )
-feat_5 = concat( S_5_0 S_4_1 S_3_2                   )
-   ....
-
-In above, all levels except feat_0 and feat_1 have the same number of features
-with similar semantic meaning. This enables training a single optical flow
-predictor module shared by levels 2,3,4,5... . For more details and evaluation
-see [1].
-
-[1] Multi-view Image Fusion, Trinidad et al. 2019
-"""
-
-from typing import List
-
-import options
-import tensorflow as tf
-
-
-def _relu(x: tf.Tensor) -> tf.Tensor:
-  return tf.nn.leaky_relu(x, alpha=0.2)
-
-
-def _conv(filters: int, name: str):
-  return tf.keras.layers.Conv2D(
-      name=name,
-      filters=filters,
-      kernel_size=3,
-      padding='same',
-      activation=_relu)
-
-
-class SubTreeExtractor(tf.keras.layers.Layer):
-  """Extracts a hierarchical set of features from an image.
-
-  This is a conventional, hierarchical image feature extractor, that extracts
-  [k, k*2, k*4... ] filters for the image pyramid where k=options.sub_levels.
-  Each level is followed by average pooling.
-
-  Attributes:
-    name: Name for the layer
-    config: Options for the fusion_net frame interpolator
-  """
-
-  def __init__(self, name: str, config: options.Options):
-    super().__init__(name=name)
-    k = config.filters
-    n = config.sub_levels
-    self.convs = []
-    for i in range(n):
-      self.convs.append(
-          _conv(filters=(k << i), name='cfeat_conv_{}'.format(2 * i)))
-      self.convs.append(
-          _conv(filters=(k << i), name='cfeat_conv_{}'.format(2 * i + 1)))
-
-  def call(self, image: tf.Tensor, n: int) -> List[tf.Tensor]:
-    """Extracts a pyramid of features from the image.
-
-    Args:
-      image: tf.Tensor with shape BATCH_SIZE x HEIGHT x WIDTH x CHANNELS.
-      n: number of pyramid levels to extract. This can be less or equal to
-       options.sub_levels given in the __init__.
-    Returns:
-      The pyramid of features, starting from the finest level. Each element
-      contains the output after the last convolution on the corresponding
-      pyramid level.
-    """
-    head = image
-    pool = tf.keras.layers.AveragePooling2D(
-        pool_size=2, strides=2, padding='valid')
-    pyramid = []
-    for i in range(n):
-      head = self.convs[2*i](head)
-      head = self.convs[2*i+1](head)
-      pyramid.append(head)
-      if i < n-1:
-        head = pool(head)
-    return pyramid
-
-
-class FeatureExtractor(tf.keras.layers.Layer):
-  """Extracts features from an image pyramid using a cascaded architecture.
-
-  Attributes:
-    name: Name of the layer
-    config: Options for the fusion_net frame interpolator
-  """
-
-  def __init__(self, name: str, config: options.Options):
-    super().__init__(name=name)
-    self.extract_sublevels = SubTreeExtractor('sub_extractor', config)
-    self.options = config
-
-  def call(self, image_pyramid: List[tf.Tensor]) -> List[tf.Tensor]:
-    """Extracts a cascaded feature pyramid.
-
-    Args:
-      image_pyramid: Image pyramid as a list, starting from the finest level.
-    Returns:
-      A pyramid of cascaded features.
-    """
-    sub_pyramids = []
-    for i in range(len(image_pyramid)):
-      # At each level of the image pyramid, creates a sub_pyramid of features
-      # with 'sub_levels' pyramid levels, re-using the same SubTreeExtractor.
-      # We use the same instance since we want to share the weights.
-      #
-      # However, we cap the depth of the sub_pyramid so we don't create features
-      # that are beyond the coarsest level of the cascaded feature pyramid we
-      # want to generate.
-      capped_sub_levels = min(len(image_pyramid) - i, self.options.sub_levels)
-      sub_pyramids.append(
-          self.extract_sublevels(image_pyramid[i], capped_sub_levels))
-    # Below we generate the cascades of features on each level of the feature
-    # pyramid. Assuming sub_levels=3, The layout of the features will be
-    # as shown in the example on file documentation above.
-    feature_pyramid = []
-    for i in range(len(image_pyramid)):
-      features = sub_pyramids[i][0]
-      for j in range(1, self.options.sub_levels):
-        if j <= i:
-          features = tf.concat([features, sub_pyramids[i - j][j]], axis=-1)
-      feature_pyramid.append(features)
-    return feature_pyramid
diff --git a/src/fusion.py b/src/fusion.py
deleted file mode 100644
index a9b572b..0000000
--- a/src/fusion.py
+++ /dev/null
@@ -1,140 +0,0 @@
-# Copyright 2022 Google LLC
-
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-
-#     https://www.apache.org/licenses/LICENSE-2.0
-
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""The final fusion stage for the film_net frame interpolator.
-
-The inputs to this module are the warped input images, image features and
-flow fields, all aligned to the target frame (often midway point between the
-two original inputs). The output is the final image. FILM has no explicit
-occlusion handling -- instead using the abovementioned information this module
-automatically decides how to best blend the inputs together to produce content
-in areas where the pixels can only be borrowed from one of the inputs.
-
-Similarly, this module also decides on how much to blend in each input in case
-of fractional timestep that is not at the halfway point. For example, if the two
-inputs images are at t=0 and t=1, and we were to synthesize a frame at t=0.1,
-it often makes most sense to favor the first input. However, this is not
-always the case -- in particular in occluded pixels.
-
-The architecture of the Fusion module follows U-net [1] architecture's decoder
-side, e.g. each pyramid level consists of concatenation with upsampled coarser
-level output, and two 3x3 convolutions.
-
-The upsampling is implemented as 'resize convolution', e.g. nearest neighbor
-upsampling followed by 2x2 convolution as explained in [2]. The classic U-net
-uses max-pooling which has a tendency to create checkerboard artifacts.
-
-[1] Ronneberger et al. U-Net: Convolutional Networks for Biomedical Image
-    Segmentation, 2015, https://arxiv.org/pdf/1505.04597.pdf
-[2] https://distill.pub/2016/deconv-checkerboard/
-"""
-
-from typing import List
-
-import options
-import tensorflow as tf
-
-
-def _relu(x: tf.Tensor) -> tf.Tensor:
-  return tf.nn.leaky_relu(x, alpha=0.2)
-
-
-_NUMBER_OF_COLOR_CHANNELS = 3
-
-
-class Fusion(tf.keras.layers.Layer):
-  """The decoder."""
-
-  def __init__(self, name: str, config: options.Options):
-    super().__init__(name=name)
-
-    # Each item 'convs[i]' will contain the list of convolutions to be applied
-    # for pyramid level 'i'.
-    self.convs: List[List[tf.keras.layers.Layer]] = []
-
-    # Store the levels, so we can verify right number of levels in call().
-    self.levels = config.fusion_pyramid_levels
-
-    # Create the convolutions. Roughly following the feature extractor, we
-    # double the number of filters when the resolution halves, but only up to
-    # the specialized_levels, after which we use the same number of filters on
-    # all levels.
-    #
-    # We create the convs in fine-to-coarse order, so that the array index
-    # for the convs will correspond to our normal indexing (0=finest level).
-    for i in range(config.fusion_pyramid_levels - 1):
-      m = config.specialized_levels
-      k = config.filters
-      num_filters = (k << i) if i < m else (k << m)
-
-      convs: List[tf.keras.layers.Layer] = []
-      convs.append(
-          tf.keras.layers.Conv2D(
-              filters=num_filters, kernel_size=[2, 2], padding='same'))
-      convs.append(
-          tf.keras.layers.Conv2D(
-              filters=num_filters,
-              kernel_size=[3, 3],
-              padding='same',
-              activation=_relu))
-      convs.append(
-          tf.keras.layers.Conv2D(
-              filters=num_filters,
-              kernel_size=[3, 3],
-              padding='same',
-              activation=_relu))
-      self.convs.append(convs)
-
-    # The final convolution that outputs RGB:
-    self.output_conv = tf.keras.layers.Conv2D(
-        filters=_NUMBER_OF_COLOR_CHANNELS, kernel_size=1)
-
-  def call(self, pyramid: List[tf.Tensor]) -> tf.Tensor:
-    """Runs the fusion module.
-
-    Args:
-      pyramid: The input feature pyramid as list of tensors. Each tensor being
-        in (B x H x W x C) format, with finest level tensor first.
-
-    Returns:
-      A batch of RGB images.
-    Raises:
-      ValueError, if len(pyramid) != config.fusion_pyramid_levels as provided in
-        the constructor.
-    """
-    if len(pyramid) != self.levels:
-      raise ValueError(
-          'Fusion called with different number of pyramid levels '
-          f'{len(pyramid)} than it was configured for, {self.levels}.')
-
-    # As a slight difference to a conventional decoder (e.g. U-net), we don't
-    # apply any extra convolutions to the coarsest level, but just pass it
-    # to finer levels for concatenation. This choice has not been thoroughly
-    # evaluated, but is motivated by the educated guess that the fusion part
-    # probably does not need large spatial context, because at this point the
-    # features are spatially aligned by the preceding warp.
-    net = pyramid[-1]
-
-    # Loop starting from the 2nd coarsest level:
-    for i in reversed(range(0, self.levels - 1)):
-      # Resize the tensor from coarser level to match for concatenation.
-      level_size = tf.shape(pyramid[i])[1:3]
-      net = tf.image.resize(net, level_size,
-                            tf.image.ResizeMethod.NEAREST_NEIGHBOR)
-      net = self.convs[i][0](net)
-      net = tf.concat([pyramid[i], net], axis=-1)
-      net = self.convs[i][1](net)
-      net = self.convs[i][2](net)
-    net = self.output_conv(net)
-    return net
diff --git a/src/init.py b/src/init.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/model.py b/src/model.py
deleted file mode 100644
index 81b7bc5..0000000
--- a/src/model.py
+++ /dev/null
@@ -1,168 +0,0 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import layers
-import math
-import numpy as np
-from tensorflow_addons import optimizers as tfa_optimizers
-
-from feature_extractor import * 
-from fusion import * 
-from options import * 
-from pyramid_flow_estimator import *  
-from util import * 
-
-def _mse_psnr(original, reconstruction,training):
-  """Calculates mse and PSNR.
-
-  If training is False, we quantize the pixel values before calculating the
-  metrics.
-
-  Args:
-    original: Image, in [0, 1].
-    reconstruction: Reconstruction, in [0, 1].
-    training: Whether we are in training mode.
-
-  Returns:
-    Tuple mse, psnr.
-  """
-  # The images/reconstructions are in [0...1] range, but we scale them to
-  # [0...255] before computing the MSE.
-  mse_per_batch = tf.reduce_mean(
-      tf.math.squared_difference(
-          (original * 255.0),
-          (reconstruction * 255.0)),
-      axis=(1, 2, 3))
-  mse = tf.reduce_mean(mse_per_batch)
-  psnr_factor = -10. / tf.math.log(10.)
-  psnr = tf.reduce_mean(psnr_factor * tf.math.log(mse_per_batch / (255.**2)))
-  return mse, psnr
-
-
-class Model(tf.Module):
-    def __init__(self,config= options.Options) :
-        super(Model, self).__init__()
-   
-        self.config=config
-        self.extract = FeatureExtractor('feat_net', self.config)
-        self.predict_flow = PyramidFlowEstimator(
-            'predict_flow', config)
-        self.fuse = Fusion('fusion', self.config)
-        self._all_trainable_variables = None
-        #self.learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
-        #    initial_learning_rate=.0001, decay_steps=750000, decay_rate=0.464158)
-        self.learning_rate_schedule = tf.keras.experimental.CosineDecay(initial_learning_rate=0.0001, decay_steps=750000, alpha=0.464158)
-        self.optimizer = tf.keras.optimizers.Adam(learning_rate= self.learning_rate_schedule)
-        #self.optimizer = tfa_optimizers.AdamW(learning_rate= self.learning_rate_schedule,weight_decay=0.0001)
-
-
-    def _iter_trainable_variables(self):
-
-        def ensure_nonempty(seq):
-            if not seq:
-                raise ValueError("No trainable variables!")
-            return seq
-
-        yield from ensure_nonempty(self.extract.trainable_variables)
-        yield from ensure_nonempty(self.predict_flow.trainable_variables)
-        yield from ensure_nonempty(self.fuse.trainable_variables)
-
-    @property
-    def all_trainable_variables(self):
-        if self._all_trainable_variables is None:
-            self._all_trainable_variables = list(self._iter_trainable_variables())
-            assert self._all_trainable_variables
-            assert len(self._all_trainable_variables) == len(self.trainable_variables)
-        return self._all_trainable_variables
-            
-    def train_step(self, img0,gt,img1):
-
-        with tf.GradientTape() as tape:
-            pred,metric = self.interpolator(img0,gt,img1)
-            loss= metric['loss']
-        var_list = self.all_trainable_variables
-        gradients = tape.gradient ( loss, var_list)
-        self.optimizer.apply_gradients(zip(gradients,var_list))
-
-        return metric
-    def write_ckpt(self, path, step):
-        """Creates a checkpoint at `path` for `step`."""
-        print('Creates a checkpoint at'+ str(path) + 'for' + str(step))
-        ckpt = tf.train.Checkpoint(model=self)
-        manager = tf.train.CheckpointManager(ckpt, path, max_to_keep=3)
-        manager.save(checkpoint_number=step)
-        return tf.train.latest_checkpoint(path)
-        
-    def load_ckpt(self, path):
-        """load a checkpoint at `path` for `step`."""
-        ckpt = tf.train.Checkpoint(model=self)
-        manager = tf.train.CheckpointManager(ckpt, path, max_to_keep=3)
-        print('load a checkpoint at'+ str(path) + 'for' + manager.latest_checkpoint)
-        return ckpt.restore(manager.latest_checkpoint).assert_existing_objects_matched()
-    @property
-    def global_step(self):
-        """Returns the global step variable."""
-        return self._optimizer.iterations
-    # Actually consist of 'calculate_flow' and 'coraseWarp_and_Refine'
-    def interpolator(self,img0,gt,img1, config, timestep=0.5):
-        #print("1. flow estimation")
-        B = img0.shape[0] # batch size 
-        image_pyramids = [
-        build_image_pyramid(img0, self.config),
-        build_image_pyramid(img1, self.config) ]
-
-        
-        feature_pyramids = [self.extract(image_pyramids[0]), self.extract(image_pyramids[1])]
- 
-
-        # Predict forward flow.
-        forward_residual_flow_pyramid = self.predict_flow(feature_pyramids[0],
-                                                    feature_pyramids[1])
-        # Predict backward flow.
-        backward_residual_flow_pyramid = self.predict_flow(feature_pyramids[1],
-                                                        feature_pyramids[0])
-
-
-        fusion_pyramid_levels = self.config.fusion_pyramid_levels
-
-        forward_flow_pyramid = flow_pyramid_synthesis(
-            forward_residual_flow_pyramid)[:fusion_pyramid_levels]
-        backward_flow_pyramid = flow_pyramid_synthesis(
-            backward_residual_flow_pyramid)[:fusion_pyramid_levels]
-
-        # We multiply the flows with t and 1-t to warp to the desired fractional time.
-        #
-        # Note: In film_net we fix time to be 0.5, and recursively invoke the interpo-
-        # lator for multi-frame interpolation. Below, we create a constant tensor of
-        # shape [B]. We use the `time` tensor to infer the batch size.
-        mid_time = tf.keras.layers.Lambda(lambda x: tf.ones_like(x) * 0.5)(timestep)
-        backward_flow = multiply_pyramid(backward_flow_pyramid, mid_time)
-        forward_flow = multiply_pyramid(forward_flow_pyramid, 1 - mid_time)
-
-        pyramids_to_warp = [
-            concatenate_pyramids(image_pyramids[0][:fusion_pyramid_levels],
-                                        feature_pyramids[0][:fusion_pyramid_levels]),
-            concatenate_pyramids(image_pyramids[1][:fusion_pyramid_levels],
-                                        feature_pyramids[1][:fusion_pyramid_levels])
-        ]
-
-        forward_warped_pyramid = pyramid_warp(pyramids_to_warp[0], backward_flow)
-        backward_warped_pyramid = pyramid_warp(pyramids_to_warp[1], forward_flow)
-
-        aligned_pyramid = concatenate_pyramids(forward_warped_pyramid,
-                                                    backward_warped_pyramid)
-        aligned_pyramid = concatenate_pyramids(aligned_pyramid, backward_flow)
-        aligned_pyramid = concatenate_pyramids(aligned_pyramid, forward_flow)
-
-        
-        prediction = self.fuse(aligned_pyramid)
-
-        output_color = prediction[..., :3]
-        outputs = {'image': output_color}
-
-        loss_l1 = tf.reduce_mean(tf.abs(output_color-gt))
-        mse,psnr = _mse_psnr(gt,output_color,False)
-        metric={'loss' : loss_l1 , 'psnr' : psnr , 'mse':mse }     
-        #return flow_list, mask_list, merged, pred
-    
-        return output_color, metric 
-    
diff --git a/src/options.py b/src/options.py
deleted file mode 100644
index 26cdb1e..0000000
--- a/src/options.py
+++ /dev/null
@@ -1,81 +0,0 @@
-# Copyright 2022 Google LLC
-
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-
-#     https://www.apache.org/licenses/LICENSE-2.0
-
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Options for the film_net video frame interpolator."""
-
-import gin.tf
-
-
-@gin.configurable('film_net')
-class Options(object):
-  """Options for the film_net video frame interpolator.
-
-  To further understand these options, see the paper here:
-  https://augmentedperception.github.io/pixelfusion/.
-
-  The default values are suitable for up to 64 pixel motions. For larger motions
-  the number of flow convolutions and/or pyramid levels can be increased, but
-  usually with the cost of accuracy on solving the smaller motions.
-
-  The maximum motion in pixels that the system can resolve is equivalent to
-  2^(pyramid_levels-1) * flow_convs[-1]. I.e. the downsampling factor times
-  the receptive field radius on the coarsest pyramid level. This, of course,
-  assumes that the training data contains such motions.
-
-  Note that to avoid a run-time error, the input image width and height have to
-  be divisible by 2^(pyramid_levels-1).
-
-  Attributes:
-    pyramid_levels: How many pyramid levels to use for the feature pyramid and
-      the flow prediction.
-    fusion_pyramid_levels: How many pyramid levels to use for the fusion module
-      this must be less or equal to 'pyramid_levels'.
-    specialized_levels: How many fine levels of the pyramid shouldn't share the
-      weights. If specialized_levels = 3, it means that two finest levels are
-      independently learned, whereas the third will be learned together with the
-      rest of the pyramid. Valid range [1, pyramid_levels].
-    flow_convs: Convolutions per residual flow predictor. This array should have
-      specialized_levels+1 items on it, the last item representing the number of
-      convs used by any pyramid level that uses shared weights.
-    flow_filters: Base number of filters in residual flow predictors. This array
-      should have specialized_levels+1 items on it, the last item representing
-      the number of filters used by any pyramid level that uses shared weights.
-    sub_levels: The depth of the cascaded feature tree each pyramid level
-      concatenates together to compute the flow. This must be within range [1,
-      specialized_level+1]. It is recommended to set this to specialized_levels
-      + 1
-    filters: Base number of features to extract. On each pyramid level the
-      number doubles. This is used by both feature extraction and fusion stages.
-    use_aux_outputs: Set to True to include auxiliary outputs along with the
-      predicted image.
-  """
-
-  def __init__(self,
-               pyramid_levels=5,
-               fusion_pyramid_levels=5,
-               specialized_levels=3,
-               flow_convs=None,
-               flow_filters=None,
-               sub_levels=4,
-               filters=16,
-               use_aux_outputs=True):
-    self.pyramid_levels = pyramid_levels
-    self.fusion_pyramid_levels = fusion_pyramid_levels
-    self.specialized_levels = specialized_levels
-    self.flow_convs = flow_convs or [4, 4, 4, 4]
-    self.flow_filters = flow_filters or [64, 128, 256, 256]
-    self.sub_levels = sub_levels
-    self.filters = filters
-    self.use_aux_outputs = use_aux_outputs
-
diff --git a/src/pyramid_flow_estimator.py b/src/pyramid_flow_estimator.py
deleted file mode 100644
index ad88088..0000000
--- a/src/pyramid_flow_estimator.py
+++ /dev/null
@@ -1,163 +0,0 @@
-# Copyright 2022 Google LLC
-
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-
-#     https://www.apache.org/licenses/LICENSE-2.0
-
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""TF2 layer for estimating optical flow by a residual flow pyramid.
-
-This approach of estimating optical flow between two images can be traced back
-to [1], but is also used by later neural optical flow computation methods such
-as SpyNet [2] and PWC-Net [3].
-
-The basic idea is that the optical flow is first estimated in a coarse
-resolution, then the flow is upsampled to warp the higher resolution image and
-then a residual correction is computed and added to the estimated flow. This
-process is repeated in a pyramid on coarse to fine order to successively
-increase the resolution of both optical flow and the warped image.
-
-In here, the optical flow predictor is used as an internal component for the
-film_net frame interpolator, to warp the two input images into the inbetween,
-target frame.
-
-[1] F. Glazer, Hierarchical motion detection. PhD thesis, 1987.
-[2] A. Ranjan and M. J. Black, Optical Flow Estimation using a Spatial Pyramid
-    Network. 2016
-[3] D. Sun X. Yang, M-Y. Liu and J. Kautz, PWC-Net: CNNs for Optical Flow Using
-    Pyramid, Warping, and Cost Volume, 2017
-"""
-
-from typing import List
-
-import options
-import util
-import tensorflow as tf
-
-
-def _relu(x: tf.Tensor) -> tf.Tensor:
-  return tf.nn.leaky_relu(x, alpha=0.2)
-
-
-class FlowEstimator(tf.keras.layers.Layer):
-  """Small-receptive field predictor for computing the flow between two images.
-
-  This is used to compute the residual flow fields in PyramidFlowEstimator.
-
-  Note that while the number of 3x3 convolutions & filters to apply is
-  configurable, two extra 1x1 convolutions are appended to extract the flow in
-  the end.
-
-  Attributes:
-    name: The name of the layer
-    num_convs: Number of 3x3 convolutions to apply
-    num_filters: Number of filters in each 3x3 convolution
-  """
-
-  def __init__(self, name: str, num_convs: int, num_filters: int):
-    super(FlowEstimator, self).__init__(name=name)
-    def conv(filters, size, name, activation=_relu):
-      return tf.keras.layers.Conv2D(
-          name=name,
-          filters=filters,
-          kernel_size=size,
-          padding='same',
-          activation=activation)
-
-    self._convs = []
-    for i in range(num_convs):
-      self._convs.append(conv(filters=num_filters, size=3, name=f'conv_{i}'))
-    self._convs.append(conv(filters=num_filters/2, size=1, name=f'conv_{i+1}'))
-    # For the final convolution, we want no activation at all to predict the
-    # optical flow vector values. We have done extensive testing on explicitly
-    # bounding these values using sigmoid, but it turned out that having no
-    # activation gives better results.
-    self._convs.append(
-        conv(filters=2, size=1, name=f'conv_{i+2}', activation=None))
-
-  def call(self, features_a: tf.Tensor, features_b: tf.Tensor) -> tf.Tensor:
-    """Estimates optical flow between two images.
-
-    Args:
-      features_a: per pixel feature vectors for image A (B x H x W x C)
-      features_b: per pixel feature vectors for image B (B x H x W x C)
-
-    Returns:
-      A tensor with optical flow from A to B
-    """
-    net = tf.concat([features_a, features_b], axis=-1)
-    for conv in self._convs:
-      net = conv(net)
-    return net
-
-
-class PyramidFlowEstimator(tf.keras.layers.Layer):
-  """Predicts optical flow by coarse-to-fine refinement.
-
-  Attributes:
-    name: The name of the layer
-    config: Options for the film_net frame interpolator
-  """
-
-  def __init__(self, name: str, config: options.Options):
-    super(PyramidFlowEstimator, self).__init__(name=name)
-    self._predictors = []
-    for i in range(config.specialized_levels):
-      self._predictors.append(
-          FlowEstimator(
-              name=f'flow_predictor_{i}',
-              num_convs=config.flow_convs[i],
-              num_filters=config.flow_filters[i]))
-    shared_predictor = FlowEstimator(
-        name='flow_predictor_shared',
-        num_convs=config.flow_convs[-1],
-        num_filters=config.flow_filters[-1])
-    for i in range(config.specialized_levels, config.pyramid_levels):
-      self._predictors.append(shared_predictor)
-
-  def call(self, feature_pyramid_a: List[tf.Tensor],
-           feature_pyramid_b: List[tf.Tensor]) -> List[tf.Tensor]:
-    """Estimates residual flow pyramids between two image pyramids.
-
-    Each image pyramid is represented as a list of tensors in fine-to-coarse
-    order. Each individual image is represented as a tensor where each pixel is
-    a vector of image features.
-
-    util.flow_pyramid_synthesis can be used to convert the residual flow
-    pyramid returned by this method into a flow pyramid, where each level
-    encodes the flow instead of a residual correction.
-
-    Args:
-      feature_pyramid_a: image pyramid as a list in fine-to-coarse order
-      feature_pyramid_b: image pyramid as a list in fine-to-coarse order
-
-    Returns:
-      List of flow tensors, in fine-to-coarse order, each level encoding the
-      difference against the bilinearly upsampled version from the coarser
-      level. The coarsest flow tensor, e.g. the last element in the array is the
-      'DC-term', e.g. not a residual (alternatively you can think of it being a
-      residual against zero).
-    """
-    levels = len(feature_pyramid_a)
-    v = self._predictors[-1](feature_pyramid_a[-1], feature_pyramid_b[-1])
-    residuals = [v]
-    for i in reversed(range(0, levels-1)):
-      # Upsamples the flow to match the current pyramid level. Also, scales the
-      # magnitude by two to reflect the new size.
-      level_size = tf.shape(feature_pyramid_a[i])[1:3]
-      v = tf.image.resize(images=2*v, size=level_size)
-      # Warp feature_pyramid_b[i] image based on the current flow estimate.
-      warped = util.warp(feature_pyramid_b[i], v)
-      # Estimate the residual flow between pyramid_a[i] and warped image:
-      v_residual = self._predictors[i](feature_pyramid_a[i], warped)
-      residuals.append(v_residual)
-      v = v_residual + v
-    # Use reversed() to return in the 'standard' finest-first-order:
-    return list(reversed(residuals))
diff --git a/src/train.py b/src/train.py
deleted file mode 100644
index 4139d8c..0000000
--- a/src/train.py
+++ /dev/null
@@ -1,105 +0,0 @@
-# coding=utf-8
-# Copyright 2023 The Google Research Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for models."""
-
-import functools
-import tensorflow as tf
-from model import *
-from vimeo_datasets import * 
-import argparse
-from tqdm import tqdm
-import wandb as wandb
-from wandb.keras import WandbCallback
-from options import Options
-wandb.login()
-run = wandb.init(project='vfi')
-parser = argparse.ArgumentParser()
-parser.add_argument('--batch_size' , default=32, type = int, help= 'batch size')
-parser.add_argument('--num_epoch' , default=300 , type = int, help= 'num_epoch')
-#parser.add_argument('--resume' , default='/home/jmw/ing/myfi/vfi/ckpt' , type = str, help= 'resume')
-parser.add_argument('--resume' , default=None , type = str, help= 'resume path')
-#parser.add_argument('--write_ckpt_dir' , default='/home/jmw/ing/myfi/vfi/ckpt' , type = str, help= 'ckpt directory path')
-args= parser.parse_args()
-
-class ModelsTest(tf.test.TestCase):
-
-  def _restore_evaluate(self, ckpt_p):
-    """Restore and evaluate a model for the given stage."""
-    print("""Restore and evaluate a model for the given stage.""")
-    config=Options()
-    model= Model(config)    
-    ckpt = tf.train.Checkpoint(model=model)
-    #ckpt.restore(ckpt_p).assert_existing_objects_matched()
-    ckpt.restore(ckpt_p).expect_partial()
-    
-    test_step = tf.function(model.interpolator)
-    test_loader=VimeoDataset(dataset_name='test',
-               path='/data/dataset/vimeo_dataset/vimeo_triplet',
-               batch_size=1,
-               shuffle=False)
-    loss=[]
-    psnr=[]
-    mse=[]
-    with tqdm(test_loader , unit ='batch' ) as tepoch :
-      for img0,gt,img1 in tepoch:
-        img0 = tf.cast(img0/255.0, dtype=tf.float32)
-        gt = tf.cast(gt/255.0, dtype=tf.float32)
-        img1 = tf.cast(img1/255.0, dtype=tf.float32)
-        _, metrics = test_step(img0,gt,img1,0.5)
-        loss.append(metrics['loss'].numpy())
-        psnr.append(metrics['psnr'].numpy())
-        mse.append(metrics['mse'].numpy())
-
-      print("Avg PSNR: {} loss: {}  MSE {}  ".format(np.mean(psnr), np.mean(loss), np.mean(mse)))
-
-  def test_train_eval(self):
-    ckpt_dir = args.resume
-    config=Options()
-    model= Model(config)
-    train_loader= VimeoDataset(dataset_name='train',
-               path='/data/dataset/vimeo_dataset/vimeo_triplet',
-               batch_size=args.batch_size,
-               mode='full',
-               shuffle=True)
-    
-    train_step = tf.function(model.train_step)
-
-    if args.resume != None :
-      print('resume',args.resume)
-      ckpt_p= args.resume
-      model.load_ckpt(path=args.resume)
-
-    wandb.init()
-    for epoch in (range(args.num_epoch)):
-      with tqdm(train_loader , unit ='batch' ) as tepoch :
-        for img0,gt,img1 in (tepoch):
-        
-          img0 = tf.cast(img0/255.0, dtype=tf.float32)
-          gt = tf.cast(gt/255.0, dtype=tf.float32)
-          img1 = tf.cast(img1/255.0, dtype=tf.float32)
-
-          metrics=train_step(img0,gt,img1)
-          tepoch.set_postfix(epoch=epoch,psnr=metrics['psnr'].numpy() , loss =metrics['loss'].numpy() ,mse =metrics['mse'].numpy()  )
-          wandb.log(metrics)
-            
-        if tf.equal(epoch % 1, 0):
-          ckpt_p = model.write_ckpt(args.write_ckpt_dir, step=epoch)
-          print(ckpt_p)
-    self._restore_evaluate(ckpt_p)
-    wandb.finish()
-
-if __name__ == "__main__":
-  tf.test.main()
diff --git a/src/util.py b/src/util.py
deleted file mode 100644
index ad250e7..0000000
--- a/src/util.py
+++ /dev/null
@@ -1,144 +0,0 @@
-# Copyright 2022 Google LLC
-
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-
-#     https://www.apache.org/licenses/LICENSE-2.0
-
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Various utilities used in the film_net frame interpolator model."""
-from typing import List
-
-from options import Options 
-import tensorflow as tf
-import tensorflow_addons.image as tfa_image
-
-
-def build_image_pyramid(image: tf.Tensor,
-                        options: Options) -> List[tf.Tensor]:
-  """Builds an image pyramid from a given image.
-
-  The original image is included in the pyramid and the rest are generated by
-  successively halving the resolution.
-
-  Args:
-    image: the input image.
-    options: film_net options object
-
-  Returns:
-    A list of images starting from the finest with options.pyramid_levels items
-  """
-  levels = options.pyramid_levels
-  pyramid = []
-  pool = tf.keras.layers.AveragePooling2D(
-      pool_size=2, strides=2, padding='valid')
-  for i in range(0, levels):
-    pyramid.append(image)
-    if i < levels-1:
-      image = pool(image)
-  return pyramid
-
-@tf.function
-def warp(image: tf.Tensor, flow: tf.Tensor) -> tf.Tensor:
-  """Backward warps the image using the given flow.
-
-  Specifically, the output pixel in batch b, at position x, y will be computed
-  as follows:
-    (flowed_y, flowed_x) = (y+flow[b, y, x, 1], x+flow[b, y, x, 0])
-    output[b, y, x] = bilinear_lookup(image, b, flowed_y, flowed_x)
-
-  Note that the flow vectors are expected as [x, y], e.g. x in position 0 and
-  y in position 1.
-
-  Args:
-    image: An image with shape BxHxWxC.
-    flow: A flow with shape BxHxWx2, with the two channels denoting the relative
-      offset in order: (dx, dy).
-  Returns:
-    A warped image.
-  """
-  # tfa_image.dense_image_warp expects unconventional negated optical flow, so
-  # negate the flow here. Also revert x and y for compatibility with older saved
-  # models trained with custom warp op that stored (x, y) instead of (y, x) flow
-  # vectors.
-  flow = -flow[..., ::-1]
-
-  # Note: we have to wrap tfa_image.dense_image_warp into a Keras Lambda,
-  # because it is not compatible with Keras symbolic tensors and we want to use
-  # this code as part of a Keras model.  Wrapping it into a lambda has the
-  # consequence that tfa_image.dense_image_warp is only called once the tensors
-  # are concrete, e.g. actually contain data. The inner lambda is a workaround
-  # for passing two parameters, e.g you would really want to write:
-  # tf.keras.layers.Lambda(tfa_image.dense_image_warp)(image, flow), but this is
-  # not supported by the Keras Lambda.
-  warped = tf.keras.layers.Lambda(
-      lambda x: tfa_image.dense_image_warp(*x))((image, flow))
-  return tf.reshape(warped, shape=tf.shape(image))
-
-
-def multiply_pyramid(pyramid: List[tf.Tensor],
-                     scalar: tf.Tensor) -> List[tf.Tensor]:
-  """Multiplies all image batches in the pyramid by a batch of scalars.
-
-  Args:
-    pyramid: Pyramid of image batches.
-    scalar: Batch of scalars.
-
-  Returns:
-    An image pyramid with all images multiplied by the scalar.
-  """
-  # To multiply each image with its corresponding scalar, we first transpose
-  # the batch of images from BxHxWxC-format to CxHxWxB. This can then be
-  # multiplied with a batch of scalars, then we transpose back to the standard
-  # BxHxWxC form.
-  return [
-      tf.transpose(tf.transpose(image, [3, 1, 2, 0]) * scalar, [3, 1, 2, 0])
-      for image in pyramid
-  ]
-
-
-def flow_pyramid_synthesis(
-    residual_pyramid: List[tf.Tensor]) -> List[tf.Tensor]:
-  """Converts a residual flow pyramid into a flow pyramid."""
-  flow = residual_pyramid[-1]
-  flow_pyramid = [flow]
-  for residual_flow in reversed(residual_pyramid[:-1]):
-    level_size = tf.shape(residual_flow)[1:3]
-    flow = tf.image.resize(images=2*flow, size=level_size)
-    flow = residual_flow + flow
-    flow_pyramid.append(flow)
-  # Use reversed() to return in the 'standard' finest-first-order:
-  return list(reversed(flow_pyramid))
-
-
-def pyramid_warp(feature_pyramid: List[tf.Tensor],
-                 flow_pyramid: List[tf.Tensor]) -> List[tf.Tensor]:
-  """Warps the feature pyramid using the flow pyramid.
-
-  Args:
-    feature_pyramid: feature pyramid starting from the finest level.
-    flow_pyramid: flow fields, starting from the finest level.
-
-  Returns:
-    Reverse warped feature pyramid.
-  """
-  warped_feature_pyramid = []
-  for features, flow in zip(feature_pyramid, flow_pyramid):
-    warped_feature_pyramid.append(warp(features, flow))
-  return warped_feature_pyramid
-
-
-def concatenate_pyramids(pyramid1: List[tf.Tensor],
-                         pyramid2: List[tf.Tensor]) -> List[tf.Tensor]:
-  """Concatenates each pyramid level together in the channel dimension."""
-  result = []
-  for features1, features2 in zip(pyramid1, pyramid2):
-    result.append(tf.concat([features1, features2], axis=-1))
-  return result
-
diff --git a/src/vimeo_datasets.py b/src/vimeo_datasets.py
deleted file mode 100644
index 88f4365..0000000
--- a/src/vimeo_datasets.py
+++ /dev/null
@@ -1,153 +0,0 @@
-import cv2
-import os
-import numpy as np
-import random
-from tensorflow.keras.utils import Sequence
-import tensorflow as tf
-import math
-AUTOTUNE = tf.data.experimental.AUTOTUNE
-cv2.setNumThreads(1)
-class VimeoDataset(Sequence):
-    def __init__(self, dataset_name, path, batch_size=32, shuffle=False, train_root='tri_trainlist.txt',test_root='tri_testlist.txt',mode='full'):
-        self.batch_size = batch_size
-        self.dataset_name = dataset_name
-        self.h = 256
-        self.w = 448
-        self.data_root = path
-        self.shuffle=shuffle
-        self.image_root = os.path.join(self.data_root, 'sequences')
-        train_fn = os.path.join(self.data_root, train_root)
-        test_fn = os.path.join(self.data_root, test_root)
-        with open(train_fn, 'r') as f:
-            self.trainlist = f.read().splitlines()
-        with open(test_fn, 'r') as f:
-            self.testlist = f.read().splitlines()        
-
-        if mode != 'full':
-            tmp = []
-            for i, value in enumerate(self.trainlist):
-                if i % 10 == 0:
-                    tmp.append(value)
-            self.trainlist = tmp
-            tmp = []
-            for i, value in enumerate(self.testlist):
-                if i % 10 == 0:
-                    tmp.append(value)
-            self.testlist = tmp
-
-        self.load_data()
-        self.on_epoch_end()
-
-    def __len__(self):
-        #return len(self.meta_data)
-        return math.ceil(len(self.meta_data) /self.batch_size)
-
-    def load_data(self):
-        if self.dataset_name != 'test':
-            self.meta_data = self.trainlist
-        else:
-            self.meta_data = self.testlist
-
-    def aug(self, img0, gt, img1, h, w):
-        ih, iw, _ = img0.shape
-        x = np.random.randint(0, ih - h + 1)
-        y = np.random.randint(0, iw - w + 1)
-        img0 = img0[x:x+h, y:y+w, :]
-        img1 = img1[x:x+h, y:y+w, :]
-        gt = gt[x:x+h, y:y+w, :]
-        return img0, gt, img1
-    
-    def on_epoch_end(self):
-        self.indices = np.arange(len(self.meta_data))
-        if self.shuffle == True:
-            np.random.shuffle(self.indices)
-    
-    def getimg(self, indices, is_training=False,h=256,w=256):
-        img0_list=[]
-        img1_list=[]
-        gt_list=[]
-        for index in indices:
-            imgpath = os.path.join(self.image_root, self.meta_data[index]) 
-            imgpaths = [imgpath + '/im1.png', imgpath + '/im2.png', imgpath + '/im3.png']
-            #image = tf.io.read_file(imgpaths[0])
-            #image = tf.image.decode_png(image)
-            #print('tf',image)
-            if is_training : 
-                img0= cv2.imread(imgpaths[0])
-                gt= cv2.imread(imgpaths[1])
-                img1= cv2.imread(imgpaths[2])
-
-                ih, iw, _ = img0.shape
-                x = np.random.randint(0, ih - h + 1)
-                y = np.random.randint(0, iw - w + 1)
-                img0 = img0[x:x+h, y:y+w, :]
-                img1 = img1[x:x+h, y:y+w, :]
-                gt = gt[x:x+h, y:y+w, :]
-
-                if random.uniform(0, 1) < 0.5:
-                    img0 = img0[:, :, ::-1]
-                    img1 = img1[:, :, ::-1]
-                    gt = gt[:, :, ::-1]
-                if random.uniform(0, 1) < 0.5:
-                    img1, img0 = img0, img1
-                if random.uniform(0, 1) < 0.5:
-                    img0 = img0[::-1]
-                    img1 = img1[::-1]
-                    gt = gt[::-1]
-                if random.uniform(0, 1) < 0.5:
-                    img0 = img0[:, ::-1]
-                    img1 = img1[:, ::-1]
-                    gt = gt[:, ::-1]
-
-                p = random.uniform(0, 1)
-                if p < 0.25:
-                    img0 = cv2.rotate(img0, cv2.ROTATE_90_CLOCKWISE)
-                    gt = cv2.rotate(gt, cv2.ROTATE_90_CLOCKWISE)
-                    img1 = cv2.rotate(img1, cv2.ROTATE_90_CLOCKWISE)
-                elif p < 0.5:
-                    img0 = cv2.rotate(img0, cv2.ROTATE_180)
-                    gt = cv2.rotate(gt, cv2.ROTATE_180)
-                    img1 = cv2.rotate(img1, cv2.ROTATE_180)
-                elif p < 0.75:
-                    img0 = cv2.rotate(img0, cv2.ROTATE_90_COUNTERCLOCKWISE)
-                    gt = cv2.rotate(gt, cv2.ROTATE_90_COUNTERCLOCKWISE)
-                    img1 = cv2.rotate(img1, cv2.ROTATE_90_COUNTERCLOCKWISE)
-                
-                #print(img0.shape)
-                img0_list.append(img0)
-                gt_list.append(gt)
-                img1_list.append(img1)
-            else :
-                img0_list.append(cv2.imread(imgpaths[0]))
-                gt_list.append(cv2.imread(imgpaths[1]))
-                img1_list.append(cv2.imread(imgpaths[2]))
-
-        #H=512
-        #W=512
-        #img0 = cv2.resize(img0,(W,H))#1k
-        #gt = cv2.resize(gt,(W,H))
-        #img1 = cv2.resize(img1,(W,H))
- 
-        #return np.concatenate((img0_list,gt_list,img1_list),0)
-        return np.array(img0_list), np.array(gt_list), np.array(img1_list)
-            
-    def __getitem__(self, idx):
-        low = idx * self.batch_size
-        # Cap upper bound at array length; the last batch may be smaller
-        # if the total number of items is not a multiple of batch size.
-        high = min(low + self.batch_size, len(self.meta_data))
-
-        indices = self.indices[low:high]                        
-        img0, gt, img1 = self.getimg(indices,is_training='train' in self.dataset_name )  
-
-
-        return img0,gt, img1
-
-if __name__ == '__main__':
-    test_loader=VimeoDataset(dataset_name='test',
-               path='/data/dataset/vimeo_dataset/vimeo_triplet',
-               shuffle=True)
-    for e in range(10):
-        for img0,gt,img1 in test_loader:
-            pass
-            print(img0, img0.shape)
\ No newline at end of file
